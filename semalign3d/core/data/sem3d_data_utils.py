"""Data loader utilities for data generated by SemAlign3D."""

import os
from typing import List, Dict, Callable, Union, Optional
import numpy as np
import torch
import torch.nn.functional as F
from torch import nn
from tqdm import tqdm

from semalign3d.utils import (
    image_processing,
    torch_utils,
    projections,
    camera_intrinsics,
    pca_utils,
)
from semalign3d.ext import projection_network
from semalign3d.core.data import raw_data_utils
from semalign3d.core import data_classes
from semalign3d.core.geometry import geom_filter


def load_depth(
    depth_folder: str, category: str, img_files: List[str], flips: List[bool]
):
    depths = []
    for flip in flips:
        for img_file in img_files:
            fn = os.path.basename(img_file).split(".")[0]
            depth_file = f"{depth_folder}/{category}/{fn}_depth.npy"
            depth = np.load(depth_file)
            # NEW for depth any: normalize and adjust z
            depth = 1 - 0.3 * (depth / np.max(depth))
            if flip:
                depth = np.fliplr(depth)
            # NOTE instead of downsizing depth we will resize and upscale attentions
            depths.append(depth)
    return depths


def load_depth_and_intrinsics_vggt(
    vggt_folder, category, img_files: List[str], flips: List[bool]
):
    depths = []
    normalized_camera_intrinsics = []
    depths_conf = []
    xyz = []
    for flip in flips:
        for img_file in img_files:
            fn = os.path.basename(img_file).split(".")[0]
            depth = torch.load(f"{vggt_folder}/{category}/{fn}_depth_orig.pth")
            depth_conf = torch.load(
                f"{vggt_folder}/{category}/{fn}_depth_conf_orig.pth"
            )
            K = torch.load(f"{vggt_folder}/{category}/{fn}_K_orig.pth")
            img_h, img_w = depth.shape
            scale = 1.0 / max(img_h, img_w)
            K_normalized = camera_intrinsics.scale_intrinsics(
                K, depth.shape, (img_h * scale, img_w * scale)
            )
            if flip:
                depth = torch.fliplr(depth)
                depth_conf = torch.fliplr(depth_conf)
            # pc_img_orig = unproject_depth_map_to_point_map(
            #     depth[None,:,:,None], torch.eye(3,4)[None], K[None]
            # )[0]  # (h, w, 3)
            xy_normalized = (
                image_processing.create_coordinate_tensor(img_h, img_w) * scale
            )
            pc_img_orig = projections.back_project_to_world(
                xy_normalized.reshape(-1, 2),
                depth.reshape(-1),
                torch.linalg.inv(K_normalized),
            ).reshape(
                img_h, img_w, 3
            )  # (h, w, 3)
            xyz.append(pc_img_orig.numpy())
            depths.append(depth.numpy())
            normalized_camera_intrinsics.append(K_normalized.numpy())
            depths_conf.append(depth_conf.numpy())
    normalized_camera_intrinsics = np.stack(normalized_camera_intrinsics)
    return depths, normalized_camera_intrinsics, depths_conf, xyz


def load_segmentation_masks(
    masks_folder: str,
    category: str,
    img_files: List[str],
    flips: List[bool],
    load_auto_masks=False,
):
    seg_masks = []
    seg_auto_masks = []
    for flip in flips:
        for img_file in img_files:
            fn = os.path.basename(img_file).split(".")[0]
            seg_mask_file = f"{masks_folder}/{category}/{fn}_masks.npy"
            seg_mask = np.load(seg_mask_file)
            if flip:
                for i in range(seg_mask.shape[0]):
                    seg_mask[i] = np.fliplr(seg_mask[i])
            seg_masks.append(seg_mask)
            if load_auto_masks:
                seg_auto_mask_file = (
                    f"{masks_folder}/{category}/{fn}_auto_masks_seg.npy"
                )
                seg_auto_mask = np.load(seg_auto_mask_file)
                if flip:
                    for i in range(seg_auto_mask.shape[0]):
                        seg_auto_mask[i] = np.fliplr(seg_auto_mask[i])
                seg_auto_masks.append(seg_auto_mask)
    return seg_masks, seg_auto_masks


def load_geo_embd(
    img_filepath: str,
    embds_folder: str,
    flip=False,
    num_patches=60,
    enable_min_max_norm=True,
    min_max_norm_epsilon=1e-10,
    alpha=0.5,
):
    """Load geo embeddings.
    Returns:
        ft_geo: tensor of shape (1, C, num_patches, num_patches), C = 3328
    """
    img_filename = os.path.basename(img_filepath).split(".")[0]
    category = raw_data_utils.img_file2category(img_filepath)
    flip_suffix = "_flip" if flip else ""
    # TODO rename dino embds to standard format so we can use spair_utils.load_img_embd
    pt_dino = f"{embds_folder}/{category}/{img_filename}_dino{flip_suffix}.pt"
    ft_dino = torch.load(pt_dino).to("cuda").detach()
    ft_dino = F.interpolate(
        ft_dino, size=(num_patches, num_patches), mode="bilinear", align_corners=False
    )

    # pt_sd = f'{geo_embds_folder}/{category}/{img_filename}_sd{flip_suffix}.pt'
    # ft_sd = torch.load(pt_sd)
    # ft_sd = spair_utils.load_img_embd(img_filepath, embds_folder_sd, pad=True, angle_deg=0, flip=int(flip)).to("cuda").detach()

    # interpolate
    # ft_sd = torch.cat([
    #     # sd_features['s3'],
    #     F.interpolate(ft_sd['s4'], size=(num_patches, num_patches), mode='bilinear', align_corners=False),
    #     F.interpolate(ft_sd['s5'], size=(num_patches, num_patches), mode='bilinear', align_corners=False),
    # ], dim=1)

    # diffusion features
    pt_sd = f"{embds_folder}/{category}/{img_filename}_sd{flip_suffix}.pt"
    ft_sd = torch.load(pt_sd)["s4"]
    # ft_sd = torch.cat([
    #     # sd_features['s3'],
    #     F.interpolate(ft_sd['s4'], size=(num_patches, num_patches), mode='bilinear', align_corners=False),
    #     F.interpolate(ft_sd['s5'], size=(num_patches, num_patches), mode='bilinear', align_corners=False),
    # ], dim=1)

    ft_sd = F.interpolate(
        ft_sd, size=(num_patches, num_patches), mode="bilinear", align_corners=False
    )
    # normalize
    if enable_min_max_norm:  # NOTE min max norm makes basically no difference
        ft_dino = torch_utils.min_max_norm(ft_dino, eps=min_max_norm_epsilon)
        ft_sd = torch_utils.min_max_norm(ft_sd, eps=min_max_norm_epsilon)
    ft_geo = torch.cat([alpha * ft_sd, (1 - alpha) * ft_dino], dim=1)
    # ft_geo = ft_dino
    return ft_geo.detach()


def load_geo_embd_aggregated(
    img_filepath: str,
    embds_folder: str,
    aggre_net: nn.Module,
    flip=False,
    num_patches=60,
    normalize=True,
):
    """Load geo embeddings.
    Returns:
        ft_geo: tensor of shape (1, C, num_patches, num_patches), C = 3328
    """
    with torch.no_grad():
        img_filename = os.path.basename(img_filepath).split(".")[0]
        category = raw_data_utils.img_file2category(img_filepath)
        flip_suffix = "_flip" if flip else ""
        # TODO rename dino embds to standard format so we can use spair_utils.load_img_embd

        # dino feature
        pt_dino = f"{embds_folder}/{category}/{img_filename}_dino{flip_suffix}.pt"
        ft_dino = torch.load(pt_dino).to("cuda").detach()
        ft_dino = F.interpolate(
            ft_dino,
            size=(num_patches, num_patches),
            mode="bilinear",
            align_corners=False,
        )

        # diffusion features
        pt_sd = f"{embds_folder}/{category}/{img_filename}_sd{flip_suffix}.pt"
        ft_sd = torch.load(pt_sd)

        # aggregate the features and apply post-processing
        desc_gathered = torch.cat(
            [
                ft_sd["s3"],
                F.interpolate(
                    ft_sd["s4"],
                    size=(num_patches, num_patches),
                    mode="bilinear",
                    align_corners=False,
                ),
                F.interpolate(
                    ft_sd["s5"],
                    size=(num_patches, num_patches),
                    mode="bilinear",
                    align_corners=False,
                ),
                ft_dino,
            ],
            dim=1,
        )
        desc = aggre_net(desc_gathered)  # 1, 768, 60, 60

    # normalize the descriptors
    if normalize:
        norms_desc = torch.linalg.norm(desc, dim=1, keepdim=True)
        desc = desc / (norms_desc + 1e-10)
    return desc


def load_geo_embds(img_files: List[str], embds_folder: str, flips=[False]):
    img_embds = []
    for flip in flips:
        for img_file in img_files:
            img_embds.append(
                load_geo_embd(
                    img_file,
                    embds_folder=embds_folder,
                    flip=flip,
                )
            )
    img_embds = torch.cat(img_embds, dim=0)
    img_embds_hat = F.normalize(img_embds)
    return img_embds, img_embds_hat


def load_geo_embds_aggregated(
    img_files: List[str], embds_folder: str, aggre_net: nn.Module, flips=[False]
):
    img_embds = []
    for flip in flips:
        for img_file in img_files:
            img_embds.append(
                load_geo_embd_aggregated(
                    img_file,
                    embds_folder=embds_folder,
                    aggre_net=aggre_net,
                    flip=flip,
                )
            )
    img_embds = torch.cat(img_embds, dim=0)
    img_embds_hat = F.normalize(img_embds)
    return img_embds, img_embds_hat


def load_kpt_embd_coords(
    img_files: List[str],
    img_new_size=768,
    embd_size=48,
    angle_deg=0,
    flip=0,
    pad=False,
    get_flipped_kpt_index: Optional[Callable[[int], int]] = None,
):
    assert flip == 0 or (
        flip == 1 and get_flipped_kpt_index is not None
    ), "if flip is 1, get_flipped_kpt_index must be set"
    img_kpt_embd_coords = []
    all_kpt_img_coords = []
    for img_file in tqdm(img_files):
        kpt_embd_coords = []
        kpt_img_coords = []
        img_anno = raw_data_utils.load_normalized_img_anno_from_img_fp(img_file)
        w, h = img_anno.img_width, img_anno.img_height
        for i, (x_img, y_img) in enumerate(img_anno.kp_xy):
            # flip if needed
            x_img = w - 1 - x_img if flip else x_img
            # pad to square & scale
            x, y = image_processing.transform_image_coords(
                x_img, y_img, w, h, img_new_size, pad
            )
            kpt_idx = img_anno.kp_ids[i]
            if flip == 1 and get_flipped_kpt_index is not None:
                kpt_idx = get_flipped_kpt_index(kpt_idx)
            # project keypoint coords to embedding coords
            x_embd, y_embd = image_processing.img_to_embedding_coords(
                x, y, img_w=img_new_size, embd_w=embd_size
            )
            assert (
                x_embd >= 0
                and x_embd < embd_size
                and y_embd >= 0
                and y_embd < embd_size
            ), f"Invalid embedding coords: ({x_embd}, {y_embd})"
            kpt_embd_coords.append([x_embd, y_embd, kpt_idx])
            kpt_img_coords.append([x_img, y_img, kpt_idx])
        img_kpt_embd_coords.append(torch.tensor(kpt_embd_coords))
        all_kpt_img_coords.append(torch.tensor(kpt_img_coords))
    return img_kpt_embd_coords, all_kpt_img_coords


def build_kpt_idx_to_kpt_embds(
    img_files: List[str],
    img_embds_hat: torch.Tensor,
    get_flipped_kpt_index: Callable[[int], int],
    img_size=960,
    embd_size=60,
    pad=True,
    flips=[False],
):
    # extract embeddings at embedding coords of keypoints
    kpt_embd_coords: List[torch.Tensor] = []
    kpt_img_coords: List[torch.Tensor] = []
    for flip in flips:
        emb_coords, img_coords = load_kpt_embd_coords(
            img_files=img_files,
            img_new_size=img_size,
            embd_size=embd_size,
            angle_deg=0,
            flip=1 if flip else 0,
            pad=pad,
            get_flipped_kpt_index=get_flipped_kpt_index,
        )
        kpt_embd_coords += emb_coords
        kpt_img_coords += img_coords

    # some basic check
    if len(flips) == 2:
        n_imgs = len(img_files)
        for i in range(n_imgs):
            if kpt_embd_coords[i].shape[0] != kpt_embd_coords[i + n_imgs].shape[0]:
                print(i, kpt_embd_coords[i].shape)

    # collect features at embeddings coords of keypoints
    max_n_kpts = 30
    kpt_idx_to_kpt_embds_list: Dict[int, List[torch.Tensor]] = {
        i: [] for i in range(max_n_kpts)
    }
    for i, embd_coords in enumerate(kpt_embd_coords):
        if embd_coords.size(0) > 0:
            kpt_embds = img_embds_hat[
                i, :, embd_coords[:, 1], embd_coords[:, 0]
            ]  # shape (C, n_kpts)
            for j, kpt_idx in enumerate(embd_coords[:, 2].tolist()):
                kpt_idx_to_kpt_embds_list[kpt_idx].append(kpt_embds[:, j])
    # kpt_idx_to_kpt_embds[kpt_index] = stacked features corresponding to keypoints with kpt_index
    kpt_idx_to_kpt_embds: Dict[int, Union[torch.Tensor, None]] = {}
    for i in range(max_n_kpts):
        if len(kpt_idx_to_kpt_embds_list[i]) > 0:
            kpt_idx_to_kpt_embds[i] = torch.stack(kpt_idx_to_kpt_embds_list[i])
        else:
            kpt_idx_to_kpt_embds[i] = None
    return kpt_idx_to_kpt_embds, kpt_embd_coords, kpt_img_coords


def load_processed_data(
    data_config: data_classes.SpairDataConfig,
    eval_config: data_classes.EvalConfig,
    get_flipped_kpt_index: Callable[[int], int],
    train: bool = True,
    average_flips_img_focal_length_inv=True,
    n_max_imgs=200,
):
    """Load pre-processed data"""
    if train:
        img_files_np_path = data_config.img_files_np_path_train
        flips = data_config.flips_train
        focal_lengths_inv_file = data_config.focal_lengths_inv_file_train
    else:
        img_files_np_path = data_config.img_files_np_path_eval
        flips = data_config.flips_eval
        focal_lengths_inv_file = data_config.focal_lengths_inv_file_eval

    img_files = np.load(img_files_np_path).tolist()
    n_orig_files = len(img_files)
    img_files = img_files[:n_max_imgs]

    # load embeddings
    if data_config.embd_type == "geo":
        img_embds, img_embds_hat = load_geo_embds(
            img_files,
            embds_folder=data_config.embds_folder,
            flips=flips,
        )
    elif "geo_agg" in data_config.embd_type:
        aggre_net = projection_network.AggregationNetwork(
            feature_dims=[640, 1280, 1280, 768], projection_dim=768, device="cuda"
        )
        aggre_net.load_pretrained_weights(torch.load(data_config.aggre_net_fp))
        img_embds, img_embds_hat = load_geo_embds_aggregated(
            img_files,
            embds_folder=data_config.embds_folder,
            aggre_net=aggre_net,
            flips=flips,
        )
    else:
        raise ValueError(f"Invalid embd_type: {data_config.embd_type}")
    img_embds = img_embds.detach().cpu()
    img_embds_hat = img_embds_hat.detach().cpu()

    # build kpt_idx_to_kpt_embds
    kpt_idx_to_kpt_embds, kpt_embd_coords, kpt_img_coords = build_kpt_idx_to_kpt_embds(
        img_files=img_files,
        img_embds_hat=img_embds_hat,
        img_size=data_config.img_size,
        embd_size=data_config.embd_size,
        pad=data_config.pad,
        flips=flips,
        get_flipped_kpt_index=get_flipped_kpt_index,  # spair_raw_data_utils.build_get_flipped_kpt_index(eval_config.category)
    )

    # load depth
    depths_conf = []
    if data_config.depth_type == "vggt":
        depths, normalized_camera_intrinsics, depths_conf, xyz = (
            load_depth_and_intrinsics_vggt(
                data_config.vggt_folder, eval_config.category, img_files, flips
            )
        )
        # NOTE the following is a quick fix for making VGGT results compatible with our code
        # TODO fix: focal_lengths_inv is no longer used
        focal_lengths_inv = (
            normalized_camera_intrinsics[:, 0, 0]
            + normalized_camera_intrinsics[:, 1, 1]
        ) / 2
    else:
        depths = load_depth(
            data_config.depth_folder, eval_config.category, img_files, flips
        )

        # reproject to point clouds
        # focal lengths
        focal_lengths_inv = np.ones(len(depths)) * 0.2  # NOTE dummy value for EVAL set
        if train:
            focal_lengths_inv = np.load(
                focal_lengths_inv_file
            )  # NOTE here we load optimised focal lengths
            focal_contain_flips = (
                focal_lengths_inv.shape[0] == len(flips) * n_orig_files
            )
            focal_lengths_inv = focal_lengths_inv.reshape(
                len(flips) if focal_contain_flips else 1, -1
            )
            focal_lengths_inv = focal_lengths_inv[:, :n_max_imgs]
            focal_lengths_inv = focal_lengths_inv.flatten()
            if focal_lengths_inv.shape[0] < len(depths):
                focal_lengths_inv = np.repeat(
                    focal_lengths_inv[None, :], len(flips), axis=0
                ).flatten()
            if average_flips_img_focal_length_inv and (len(flips) > 1):
                n_imgs_per_flip = len(depths) // len(flips)
                focal_lengths_inv = np.mean(
                    focal_lengths_inv.reshape(-1, n_imgs_per_flip), axis=0
                )
                # repeat
                focal_lengths_inv = np.repeat(
                    focal_lengths_inv[None, :], len(flips), axis=0
                ).flatten()
        normalized_camera_intrinsics = np.stack(
            [
                camera_intrinsics.get_normalized_calibration_matrix_np(
                    depths[i].shape[0], depths[i].shape[1], 1 / focal_lengths_inv[i]
                )
                for i in range(len(depths))
            ]
        )

        # xyz
        xyz = []
        for i, depth in enumerate(depths):
            v0 = projections.back_project_depth_image(
                depth, focal_length=1 / focal_lengths_inv[i]
            )  # (h, w, 3)
            xyz.append(v0)

    # load segmentation masks
    seg_masks, seg_auto_masks = load_segmentation_masks(
        data_config.masks_folder, eval_config.category, img_files, flips
    )

    # compute n_max_kpts
    n_max_kpts, unused_kpts = raw_data_utils.compute_n_max_kpts(kpt_img_coords)

    processed_data = data_classes.SpairProcessedData(
        img_files=img_files,
        img_embds=img_embds,
        img_embds_hat=img_embds_hat,
        kpt_idx_to_kpt_embds_hat=kpt_idx_to_kpt_embds,
        kpt_embd_coords=kpt_embd_coords,
        kpt_img_coords=kpt_img_coords,
        depths=depths,
        xyz=xyz,
        seg_masks=seg_masks,
        seg_auto_masks=seg_auto_masks,
        n_max_kpts=n_max_kpts,
        unused_kpts=unused_kpts,
        focal_lengths_inv=focal_lengths_inv,
        normalized_camera_intrinsics=normalized_camera_intrinsics,
        depths_conf=depths_conf,
    )

    return processed_data


def load_sparse_pc(
    paths: data_classes.SemAlign3DPaths,
    category: str,
    geom_stats_suffix: str = "",
):
    out_dir = os.path.join(paths.initial_pc_base_dir, category)
    prefix = "sparse_pc"
    suffix = geom_stats_suffix + paths.suffix
    sparse_pc_keys = ["xyz", "fts_hat", "attn_mean", "attn_sd"]
    sparse_pc_dict = {}
    for k in sparse_pc_keys:
        pt_file = os.path.join(out_dir, f"{prefix}_{k}{suffix}.pt")
        sparse_pc_dict[k] = torch.load(pt_file)
    return data_classes.SparsePC(**sparse_pc_dict)


def load_dense_pc(
    paths: data_classes.SemAlign3DPaths,
    category: str,
    cutoff_thresh: float = 0.0005,
):
    out_dir = os.path.join(paths.initial_dense_pc_base_dir, category)
    keys = [
        "centers",
        "fts_hat",
        "weights",
        "attn_mean",
        "attn_var",
        "tet_weights",
        "bary_coords",
        "anchor_xyz",
        "anchor_tet_combinations",
    ]
    data = {}
    for k in keys:
        pt_file = os.path.join(out_dir, f"clustered_pc_{k}{paths.suffix}.pt")
        data[k] = torch.load(pt_file)

    dense_pc_weights = data["weights"]
    dense_pc_mask = dense_pc_weights > cutoff_thresh

    dense_pc_xyz = data["centers"][dense_pc_mask]
    dense_pc_fts_hat = data["fts_hat"][dense_pc_mask]
    dense_pc_attn_mean = data["attn_mean"][dense_pc_mask]
    dense_pc_attn_var = data["attn_var"][dense_pc_mask]
    dense_pc_tet_weights = data["tet_weights"][dense_pc_mask]
    dense_pc_bary_coords = data["bary_coords"][dense_pc_mask]

    anchor_xyz = data["anchor_xyz"]
    anchor_tet_combinations = data["anchor_tet_combinations"]

    # eig vectors
    # for convenience, we apply PCA to the point cloud for visualization
    (
        pc_fts_pca,
        pc_fts_mean,
        pc_fts_std,
        pc_fts_sorted_eigenvalues,
        pc_fts_sorted_eigenvectors,
    ) = pca_utils.pca(dense_pc_fts_hat)
    dense_pc_color, dense_pc_color_max, dense_pc_color_min = pca_utils.tensor_to_rgb(
        pc_fts_pca[:, :3]
    )

    closest_to_anchor = torch.cdist(anchor_xyz, dense_pc_xyz).min(dim=1).indices

    return data_classes.DensePC(
        xyz=dense_pc_xyz,
        fts_hat=dense_pc_fts_hat,
        attn_mean=dense_pc_attn_mean,
        attn_var=dense_pc_attn_var,
        color=dense_pc_color,
        bary_coords=dense_pc_bary_coords,
        tet_weights=dense_pc_tet_weights,
        anchor_xyz=anchor_xyz,
        anchor_tet_combinations=anchor_tet_combinations,
        closest_to_anchor=closest_to_anchor,
        pc_fts_pca=pc_fts_pca,
        pc_fts_mean=pc_fts_mean,
        pc_fts_std=pc_fts_std,
        pc_fts_sorted_eigenvalues=pc_fts_sorted_eigenvalues,
        pc_fts_sorted_eigenvectors=pc_fts_sorted_eigenvectors,
        cmax=dense_pc_color_max,
        cmin=dense_pc_color_min,
    )


def load_data(
    category: str,
    paths: data_classes.SemAlign3DPaths,
    embd_type="geo_agg",  # "geo"
    load_geom_stats: bool = True,
    load_geom_stats_suffix: str = "",
    do_load_sparse_pc: bool = True,
    sparse_pc_suffix: str = "",
    do_load_dense_pc: bool = True,
    n_max_imgs=200,
    use_vggt: bool = False,
    use_seg_inter: bool = False,
    cutoff_thresh=0.0005,
):
    # load data
    spair_config = {
        "dataset": {
            "spair_data_folder": paths.spair_data_dir,
            "embds_folder": paths.embds_folder,
            "vggt_folder": paths.vggt_dir,
            "img_files_np_path_train": f"{paths.img_file_splits_dir}/{category}_img_files_trn.npy",
            "img_files_np_path_eval": f"{paths.img_file_splits_dir}/{category}_img_files_test.npy",
            "depth_folder": paths.depths_dir,
            "masks_folder": (
                paths.seg_masks_dir if not use_seg_inter else paths.seg_masks_inter_dir
            ),
            "focal_lengths_inv_file_train": f"{paths.img_focal_lengths_opt_dir}/{category}_focal_lengths_inv_opt.npy",
            # TODO focal_lengths_inv_file_eval is not used => remove (acts as placeholder here)
            "focal_lengths_inv_file_eval": f"{paths.img_focal_lengths_opt_dir}/{category}_focal_lengths_inv_opt.npy",
            "img_size": 960,
            "embd_size": 60,
            "pad": True,
            "flips_train": [False, True],
            "flips_eval": [False],
            "aggre_net_fp": paths.aggre_net_ckpt_path,
            "embd_type": embd_type,
            "depth_type": "vggt" if use_vggt else "depth_any",
        },
        # TODO category should not be in evaluation, evaluation prop should be removed
        "evaluation": {"category": category},
    }

    data_config = data_classes.SpairDataConfig(**spair_config["dataset"])
    eval_config = data_classes.EvalConfig(**spair_config["evaluation"])

    get_flipped_kpt_index = raw_data_utils.build_get_flipped_kpt_index(
        paths.dataset_name, eval_config.category
    )
    processed_data_train = load_processed_data(
        data_config,
        eval_config,
        get_flipped_kpt_index,
        train=True,
        n_max_imgs=n_max_imgs,
    )
    processed_data_eval = load_processed_data(
        data_config,
        eval_config,
        get_flipped_kpt_index,
        train=False,
        n_max_imgs=n_max_imgs,
    )

    # TODO should be a simple loading function
    # kpt_fts = sparse_pc_features.compute_keypoint_features(
    #     processed_data_train,
    #     n_max_kpts=processed_data_train.n_max_kpts,
    # )

    full_data = data_classes.SpairFullData(
        processed_data_train=processed_data_train,
        processed_data_eval=processed_data_eval,
        keypoint_features=None,
    )

    # if data is not loaded, set to None
    geom_relation_combinations_partial = None
    geom_stats_partial = None
    kpt_xyz_obj_orig = None
    used_kpts_mask = None
    dense_pc = None
    n_max_kpts = None
    unused_kpts = None

    # load geom stats
    if load_geom_stats:
        geom_stats_partial = torch.load(
            f"{paths.geom_stats_dir}/{category}_geom_stats{load_geom_stats_suffix}{paths.suffix}.pt",
            weights_only=False,
        )
        geom_relation_combinations_partial = torch.load(
            f"{paths.geom_stats_dir}/{category}_geom_combinations{load_geom_stats_suffix}{paths.suffix}.pt",
            weights_only=False,
        )
        geom_stats_partial, geom_relation_combinations_partial = (
            geom_filter.filter_valid_stats(
                geom_stats_partial, geom_relation_combinations_partial
            )
        )
        (
            geom_relation_combinations_partial,
            geom_stats_partial,
            unused_kpts,
            n_max_kpts,
        ) = geom_filter.remove_unused_keypoints(
            kpt_img_coords=full_data.processed_data_train.kpt_img_coords,
            geom_comb=geom_relation_combinations_partial,
            geom_stats=geom_stats_partial,
        )
        used_kpts_mask = torch.ones(n_max_kpts).bool()
        used_kpts_mask[unused_kpts] = False

    # load point cloud initial guess
    if load_geom_stats and do_load_sparse_pc:
        sparse_pc = load_sparse_pc(
            paths=paths,
            category=category,
            geom_stats_suffix=sparse_pc_suffix,
        )
        # TODO fix
        # make it compatible with our legacy code
        kpt_xyz_obj_orig = sparse_pc.xyz
        full_data.keypoint_features = data_classes.SparsePCFeatures(
            kpt_features_attn_avg=sparse_pc.attn_mean,
            kpt_features_attn_sd=sparse_pc.attn_sd,
            kpt_features_avg_hat=sparse_pc.fts_hat,
        )
        kpt_xyz_obj_orig = kpt_xyz_obj_orig[:n_max_kpts, :]

    # Empty CUDA cache
    torch.cuda.empty_cache()
    if load_geom_stats and do_load_sparse_pc and do_load_dense_pc:
        dense_pc = load_dense_pc(paths, category, cutoff_thresh)

    # Empty CUDA cache
    torch.cuda.empty_cache()

    sem3d_data = data_classes.SemAlign3DData(
        # TODO full data does not make any sense here, split it up
        full_data=full_data,
        dense_pc=dense_pc,
        geom_stats_partial=geom_stats_partial,
        geom_relation_combinations_partial=geom_relation_combinations_partial,
        n_max_kpts=n_max_kpts,
        unused_kpts=unused_kpts,
        kpt_xyz_obj_orig=kpt_xyz_obj_orig,
        used_kpts_mask=used_kpts_mask,
    )

    return sem3d_data


def load_gt_kpts_data_with_optimised_depth(
    processed_data: data_classes.SpairProcessedData,
    base_dir: str,
    suffix: str,
    n_verts_max: int = 25,
    use_obj_xyz=False,
    show_progress_bar=False,
):
    n_imgs = len(processed_data.xyz)
    kpt_xyz_opt_all = torch.full((n_imgs, n_verts_max, 3), fill_value=torch.nan)
    n_img_files = len(processed_data.img_files)
    for img_idx in tqdm(range(n_imgs), disable=not show_progress_bar):
        img_name = os.path.basename(
            processed_data.img_files[img_idx % n_img_files]
        ).split(".")[0]
        if img_idx >= n_img_files:
            img_name += f"_{img_idx // n_img_files}"
        if not use_obj_xyz:
            k = torch.from_numpy(
                processed_data.normalized_camera_intrinsics[img_idx]
            ).float()
            k_inv = camera_intrinsics.invert_intrinsics_batch(k[None, :])
            img_shape = torch.tensor(processed_data.xyz[img_idx].shape[:2])
            kpt_xy = processed_data.kpt_img_coords[img_idx][:, :2].clone()
            kpt_xy_normalized = kpt_xy / torch.max(img_shape)
            kpt_labels = processed_data.kpt_img_coords[img_idx][:, 2].long()
            kpt_depths_opt = torch.load(
                f"{base_dir}/{img_name}_kpt_depths{suffix}.pt"
            ).cpu()
            kpt_xyz_opt = projections.back_project_kpts_from_K_inv_normalized(
                K_inv_normalized=k_inv,
                kpt_xy_normalized=kpt_xy_normalized[None, :],
                kpt_depth=kpt_depths_opt[None, :],
            )[0]
            kpt_xyz_opt_all[img_idx, kpt_labels, :] = kpt_xyz_opt
        else:
            kpt_xyz_obj_opt = torch.load(
                f"{base_dir}/{img_name}_kpt_xyz_obj{suffix}.pt"
            ).cpu()
            kpt_xyz_opt_all[img_idx, : kpt_xyz_obj_opt.shape[0], :] = kpt_xyz_obj_opt
    vertex_mask = torch_utils.generate_is_not_nan_mask(kpt_xyz_opt_all)
    return data_classes.GtKptsData(
        xyz=kpt_xyz_opt_all,
        vertex_mask=vertex_mask,
    )
